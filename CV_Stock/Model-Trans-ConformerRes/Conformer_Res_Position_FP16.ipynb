{"cells":[{"cell_type":"markdown","metadata":{"id":"iBD1aabnrVJw"},"source":["- seperate do and dc model\n","    - solve problem that dos are large while dcs are small\n","    - result: not help so much, since the mean of do is higher than dc\n","- prediction transform with predicted value of last k train data\n","    - solve problem that dos are large while dcs are small\n","    - help\n","- chenge do to o-c\n","    - use the latest data of do\n","- indicators"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import sys\n","sys.path.append('../')\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.preprocessing import MinMaxScaler\n","import yfinance as yf\n","import numpy as np\n","import os\n","import pickle\n","from torchaudio.models import Conformer\n","import math\n","from torch import nn, Tensor\n","from tqdm import tqdm\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import DataLoader, TensorDataset\n","from sklearn.preprocessing import Normalizer, StandardScaler\n","from einops.layers.torch import Rearrange, Reduce\n","from utils import *\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","num_class = 2\n","stock_symbol = '5871.TW'\n","end_date = '2024-12-31'\n","init = True"]},{"cell_type":"markdown","metadata":{"id":"2dlPDr1feNdw"},"source":["# Init"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["with open('./DataLoader/dataloader.pk', 'rb') as f:\n","    data = pickle.load(f)\n","dataloader_train = data['trainloader']\n","dataloader_valid = data['validloader']\n","# dataloader_test = data['testloader']"]},{"cell_type":"markdown","metadata":{"id":"N2GtucuTfVrD"},"source":["# Define model"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":1477,"status":"ok","timestamp":1708508307622,"user":{"displayName":"Kuo Jacob","userId":"03725230422177139348"},"user_tz":-480},"id":"bBbKgszAfVrF"},"outputs":[],"source":["# https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n","class PositionalEncoding(nn.Module):\n","\n","    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n","        super().__init__()\n","        self.dropout = nn.Dropout(p=dropout)\n","\n","        position = torch.arange(max_len).unsqueeze(1)\n","        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n","        pe = torch.zeros(max_len, 1, d_model)\n","        pe[:, 0, 0::2] = torch.sin(position * div_term)\n","        pe[:, 0, 1::2] = torch.cos(position * div_term)\n","        self.register_buffer('pe', pe)\n","\n","    def forward(self, x: Tensor) -> Tensor:\n","        \"\"\"\n","        Arguments:\n","            x: Tensor, shape ``[seq_len, batch_size, embedding_dim]``\n","        \"\"\"\n","        x = x + self.pe[:x.size(0)]\n","        return self.dropout(x)\n","\n","# https://zhuanlan.zhihu.com/p/348849092\n","class PatchEmbedding(nn.Module):\n","    def __init__(self, in_channels: int = 3, patch_size: int = 16, emb_size: int = 768):\n","        self.patch_size = patch_size\n","        super().__init__()\n","        self.projection = nn.Sequential(\n","            # 在s1 x s2切片中分解图像并将其平面化\n","            Rearrange('b c (h s1) (w s2) -> b (h w) (s1 s2 c)', s1=patch_size, s2=patch_size),\n","            nn.Linear(patch_size * patch_size * in_channels, emb_size)\n","        )\n","                \n","    def forward(self, x: Tensor) -> Tensor:\n","        x = self.projection(x)\n","        return x\n","\n","\n","# https://medium.com/ching-i/%E5%8D%B7%E7%A9%8D%E7%A5%9E%E7%B6%93%E7%B6%B2%E7%B5%A1-cnn-%E7%B6%93%E5%85%B8%E6%A8%A1%E5%9E%8B-googlelenet-resnet-densenet-with-pytorch-code-1688015808d9\n","class bottleneck_block(nn.Module):\n","    # 輸出通道乘的倍數\n","    expansion = 4\n","\n","    def __init__(self, in_channels, out_channels, stride, downsample):\n","        super(bottleneck_block, self).__init__()      \n","        self.conv1 = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=1, bias=False)\n","        self.bn1 = nn.BatchNorm2d(out_channels)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.conv2 = nn.Conv2d(in_channels=out_channels, out_channels=out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n","        self.bn2 = nn.BatchNorm2d(out_channels)\n","        self.conv3 = nn.Conv2d(in_channels=out_channels, out_channels=out_channels * self.expansion, kernel_size=1, bias=False)\n","        self.bn3 = nn.BatchNorm2d(out_channels * self.expansion)\n","\n","        # 在 shortcut 時，若維度不一樣，要更改維度\n","        self.downsample = downsample \n","\n","\n","    def forward(self, x):\n","        residual = x\n","\n","        out = self.conv1(x)\n","        out = self.bn1(out)\n","        out = self.relu(out)\n","        out = self.conv2(out)\n","        out = self.bn2(out)\n","        out = self.relu(out)\n","        out = self.conv3(out)\n","        out = self.bn3(out)\n","\n","        if self.downsample is not None:\n","            residual = self.downsample(x)\n","\n","        out += residual\n","        out = self.relu(out)\n","\n","        return out\n","    \n","class Res_Conformer_Unet(nn.Module):\n","    def __init__(self, net_block, layers, num_class, conformer = False, res = True):\n","        super(Res_Conformer_Unet, self).__init__()\n","\n","        # =======\n","        # Unet\n","        self.in_channels = 64\n","        self.conv1 = nn.Conv2d(in_channels=5, out_channels=64, kernel_size=7, stride=2, padding=3, bias=False)\n","        self.bn1 = nn.BatchNorm2d(64)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.maxpooling = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n","\n","        self.layer1 = self.net_block_layer(net_block, 64, layers[0])\n","        self.layer2 = self.net_block_layer(net_block, 128, layers[1], stride=2)\n","        self.layer3 = self.net_block_layer(net_block, 256, layers[2], stride=2)\n","        self.layer4 = self.net_block_layer(net_block, 512, layers[3], stride=2)\n","        \n","        self.avgpooling = nn.AvgPool2d(3, stride=1)        \n","        self.relu = nn.ReLU()\n","        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2, return_indices=True)\n","        \n","        self.fc1 = nn.Linear(2048*2*2, 128)\n","        self.fc2 = nn.Linear(128, num_class)\n","        self.ln1 = nn.LayerNorm((5, 100, 100))\n","        \n","        \n","        # =======\n","        # Conformer\n","        self.positional_encode = PositionalEncoding(100)\n","        self.patch_embedding = PatchEmbedding(in_channels=5, patch_size=10, emb_size=500)\n","        self.conformer = Conformer(\n","            input_dim=500,\n","            num_heads=5,\n","            ffn_dim=128,\n","            num_layers=6,\n","            depthwise_conv_kernel_size=31)\n","\n","\n","    def net_block_layer(self, net_block, out_channels, num_blocks, stride=1):\n","        downsample = None\n","\n","      # 在 shortcut 時，若維度不一樣，要更改維度\n","        if stride != 1 or self.in_channels != out_channels * net_block.expansion:\n","            downsample = nn.Sequential(nn.Conv2d(self.in_channels, out_channels * net_block.expansion, kernel_size=1, stride=stride, bias=False),\n","                      nn.BatchNorm2d(out_channels * net_block.expansion))\n","\n","        layers = []\n","        layers.append(net_block(self.in_channels, out_channels, stride, downsample))\n","        if net_block.expansion != 1:\n","            self.in_channels = out_channels * net_block.expansion\n","        else:\n","            self.in_channels = out_channels\n","\n","        for i in range(1, num_blocks):\n","            layers.append(net_block(self.in_channels, out_channels, 1, None))\n","\n","        return nn.Sequential(*layers)\n","    \n","    def forward(self, x):\n","        \"\"\"\n","        Input scale: (0, 255)\n","        Output scale: (0, 255)\n","        \"\"\"\n","        \n","        x_i = x.clone()\n","        x_s = x.size()\n","        # =======\n","        # Conformer\n","        # x = x.view(x_s[0], x_s[1] * x_s[3], x_s[2])        \n","        # x = self.positional_encode(x)\n","        x = self.patch_embedding(x)\n","        lengths = torch.tensor([x.shape[1] for i in range(len(x))]).to(device)\n","        x, len_ = self.conformer(x, lengths)\n","        x = x.permute(0, 2, 1).view(x_s)\n","        \n","        x = self.ln1(x)\n","        x = x + x_i\n","        \n","        # =======\n","        # Res\n","        x = self.conv1(x)\n","        x = self.bn1(x)\n","        x = self.relu(x)\n","        x = self.maxpooling(x)\n","        x = self.layer1(x)\n","        x = self.layer2(x)\n","        x = self.layer3(x)\n","        x = self.layer4(x)\n","        x = self.avgpooling(x)\n","        x = x.view(x.size(0), -1)  # Flatten the tensor\n","        x = self.fc1(x)\n","        x = self.relu(x)\n","        x = self.fc2(x)\n","\n","        return x\n","\n"]},{"cell_type":"markdown","metadata":{"id":"24AwH-nhes4f"},"source":["# Train"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Accelerating\n"]},{"data":{"text/plain":["'\\nx = 123\\ndef global_var():\\n    global x\\n    x = \"awesome\"\\n    print(x)\\nprint(x)\\nglobal_var()\\n'"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["# !pip install accelerate==0.2.0\n","fp16_training = True\n","\n","if fp16_training:\n","    print('Accelerating')\n","    from accelerate import Accelerator\n","    accelerator = Accelerator()\n","    device = accelerator.device\n","\"\"\"\n","x = 123\n","def global_var():\n","    global x\n","    x = \"awesome\"\n","    print(x)\n","print(x)\n","global_var()\n","\"\"\""]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["\n","# Instantiate the model\n","if fp16_training:\n","    model = Res_Conformer_Unet(bottleneck_block, [3, 4, 23, 3], num_class)\n","else:\n","    model = Res_Conformer_Unet(bottleneck_block, [3, 4, 23, 3], num_class).to(device)"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Init model\n","Last train epoch: 0  Last train lr: 0.001   Min val loss: 10000.0\n"]}],"source":["if os.path.exists(f'Temp//Conformer_{stock_symbol}_LastTrainInfo.pk'):\n","    if init:\n","        print(\"Init model\")\n","        lr = 0.001\n","        last_epoch = 0\n","        min_val_loss = 10000.0\n","        loss_train = []\n","        loss_valid = []\n","    else:\n","        print('Load from last train epoch')\n","        with open(f'Temp//Conformer_{stock_symbol}_LastTrainInfo.pk', 'rb') as f:\n","            last_train_info = pickle.load(f)\n","        lr = last_train_info['lr']\n","        last_epoch = last_train_info['epoch']\n","        min_val_loss = last_train_info['min val loss']\n","        model.load_state_dict(torch.load(f'Temp//Conformer_{stock_symbol}_checkpoint_LastTrainModel.pt'))\n","        with open(f'Temp//Conformer_{stock_symbol}_TrainValHistLoss.pk', 'rb') as f:\n","            loss_train_val = pickle.load(f)\n","        loss_train = loss_train_val['train']\n","        loss_valid = loss_train_val['valid']\n","else:\n","    print(\"Init model\")\n","    lr = 0.001\n","    last_epoch = 0\n","    min_val_loss = 10000.0\n","    loss_train = []\n","    loss_valid = []\n","    \n","print(f'Last train epoch: {last_epoch}  '\n","        f'Last train lr: {lr}   '\n","        f'Min val loss: {min_val_loss}')"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Accelerate Prepare\n"]},{"name":"stderr","output_type":"stream","text":[" 42%|████▏     | 27/65 [00:23<00:28,  1.35it/s]"]}],"source":["import torch.optim as optim\n","import pickle\n","\n","criterion = nn.MSELoss()\n","optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=0.00001)\n","scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=len(dataloader_train)*10, gamma=0.9)        \n","\n","if fp16_training:\n","    print('Accelerate Prepare')\n","    model, optimizer, dataloader_train, dataloader_valid, scheduler = \\\n","        accelerator.prepare(model, optimizer, dataloader_train, dataloader_valid, scheduler)\n","\n","num_epochs = 1500\n","for epoch in range(last_epoch, num_epochs):\n","    # Training phase\n","    model.train()\n","    loss_train_e = 0\n","    for batch_x, batch_y in tqdm(dataloader_train):\n","        if not fp16_training:\n","            batch_x = batch_x.to(device)\n","            batch_y = batch_y.to(device)\n","        optimizer.zero_grad()\n","        outputs = model(batch_x)\n","\n","        # Loss\n","        loss = criterion(outputs, batch_y)\n","        if fp16_training:\n","            accelerator.backward(loss)\n","        else:\n","            loss.backward()\n","        optimizer.step()\n","        scheduler.step()\n","        loss_train_e += loss.item()\n","        \n","    loss_train_e /= len(dataloader_train)\n","    loss_train.append(loss_train_e)\n","    \n","    loss_valid_e = 0\n","    with torch.no_grad():\n","        model.eval()\n","        for batch_x_val, batch_y_val in tqdm(dataloader_valid):\n","            if not fp16_training:\n","                batch_x_val = batch_x_val.to(device)\n","                batch_y_val = batch_y_val.to(device)\n","            outputs_val = model(batch_x_val)\n","            loss = criterion(outputs_val, batch_y_val)\n","            loss_valid_e += loss.item()\n","        loss_valid_e /= len(dataloader_valid)\n","        loss_valid.append(loss_valid_e)\n","            \n","        torch.save(model.state_dict(), f'Temp/Conformer_{stock_symbol}_checkpoint_LastTrainModel.pt')\n","        if loss_valid_e < min_val_loss:\n","            min_val_loss = loss_valid_e\n","            print(f'New best model found in epoch {epoch} with val loss: {min_val_loss}')\n","            torch.save(model.state_dict(), f'ConformerResult/Conformer_{stock_symbol}_best_model.pt')            \n","        if epoch % 50 == 0:\n","            pass\n","            # torch.save(model, f'ConformerResult/Conformerr_{stock_symbol}_checkpoint_{epoch}.pt')\n","            \n","    with open(f'Temp/Conformer_{stock_symbol}_TrainValHistLoss.pk', 'wb') as f:\n","        pickle.dump({'train': loss_train, 'valid': loss_valid}, f)\n","    with open(f'Temp/Conformer_{stock_symbol}_LastTrainInfo.pk', 'wb') as f:\n","        pickle.dump({'min val loss': min_val_loss, 'epoch': epoch, 'lr': optimizer.param_groups[0]['lr']}, f)\n","        \n","    # Print statistics\n","    print(f'Epoch [{epoch}/{num_epochs}]',\n","        f'Training Loss: {loss_train_e:.10f}',\n","        f'Valid Loss: {loss_valid_e:.10f}')"]},{"cell_type":"markdown","metadata":{"id":"gdsTCRhq_a_O"},"source":["# Validate Model"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1708508308097,"user":{"displayName":"Kuo Jacob","userId":"03725230422177139348"},"user_tz":-480},"id":"Xr_xzxUygrCM"},"outputs":[],"source":["def load_model():\n","    import torch\n","    model = torch.load(f'ConformerResult/Conformer_{stock_symbol}_best_model.pt')\n","    return model\n","model = load_model()"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":46476,"status":"ok","timestamp":1708508354571,"user":{"displayName":"Kuo Jacob","userId":"03725230422177139348"},"user_tz":-480},"id":"-Vxhjr2bfVrJ"},"outputs":[],"source":["\n","import gc\n","def test():\n","    dataloader = dataloader_test\n","\n","    model.eval()\n","    s_pred = []\n","    s_true = []\n","    for x, y in tqdm(dataloader):\n","        y_pred = model(x)\n","        s_pred.append(y_pred.detach())\n","        s_true.append(y)\n","    y_pred_tensor = torch.concat(s_pred)\n","    y_test_tensor = torch.concat(s_true)\n","    accuracy = (torch.sign(y_pred_tensor) == torch.sign(y_test_tensor)).sum() / len(y_test_tensor)\n","    return y_pred_tensor, accuracy\n","\n","y_pred, acc = test()\n","print(acc)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":18330,"status":"ok","timestamp":1708508372899,"user":{"displayName":"Kuo Jacob","userId":"03725230422177139348"},"user_tz":-480},"id":"wMKUVbcfj8zX"},"outputs":[],"source":["# Derive y_pred and y_train_pred of shape(N, 2) and numpy type\n","\n","y_pred_numpy = y_pred.cpu().numpy()\n","\n","# predict with train set\n","y_train_pred = model(torch.tensor(X[-100:], dtype = torch.float32))\n","y_train_numpy = y_train_pred.detach().cpu().numpy()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iKLtCWerEIac"},"outputs":[],"source":["import pandas as pd\n","from sklearn.preprocessing import StandardScaler\n","\n","# Scaling\n","prediction = pd.DataFrame(y_pred_numpy)\n","scaler = StandardScaler()\n","scaler.fit(y_train_numpy)\n","prediction = pd.DataFrame(scaler.transform(prediction))\n","\n","# Get the predicted price of O and C and Prediction merge with complete data\n","prediction.columns = ['pred_do_1', 'pred_dc_1']\n","prediction['Date'] = date\n","\n","true_and_pred = pd.merge(df.reset_index(), prediction, on = 'Date', how = 'left')\n","true_and_pred['pred_o'] = (true_and_pred['Open'] * (1 + true_and_pred['pred_do_1'])).shift(1)\n","true_and_pred['pred_c'] = (true_and_pred['Close'] * (1 + true_and_pred['pred_dc_1'])).shift(1)\n","true_and_pred['pred_oc'] = true_and_pred['pred_c'] - true_and_pred['pred_o']\n","true_and_pred['true_oc'] = true_and_pred['Close'] - true_and_pred['Open']\n","\n","# Backtest\n","asset_list = []\n","df_backtest = true_and_pred[['Open', 'Close', 'true_oc', 'pred_oc']].dropna()\n","asset = 1\n","for index, (o, c, true, pred) in df_backtest.iterrows():\n","    if pred > 0:\n","        returns = true/o\n","        asset *= (1 + returns)\n","    asset_list.append(asset)\n","\n","print(asset)\n","plt.plot(asset_list, label = 'resnet')\n","plt.plot(df_backtest.reset_index()['Close']/df_backtest['Close'].iloc[0], label = 'buy hold')\n","plt.legend()\n","plt.savefig('/ConformerResult/test_backtest.jpg')\n","# plt.show()"]}],"metadata":{"colab":{"collapsed_sections":["2dlPDr1feNdw","N2GtucuTfVrD","24AwH-nhes4f"],"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":0}
